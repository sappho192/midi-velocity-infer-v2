{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle as pkl\n",
    "import keras\n",
    "from keras.utils import plot_model\n",
    "\n",
    "import pydot as pyd\n",
    "from keras.utils.vis_utils import plot_model, model_to_dot\n",
    "keras.utils.vis_utils.pydot = pyd\n",
    "import tensorflow as tf\n",
    "\n",
    "import os, glob\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AMOUNT = '-small'\n",
    "# AMOUNT = '-medium'\n",
    "AMOUNT = ''\n",
    "\n",
    "root_path = '/home/tikim/code/midi-velocity-infer'\n",
    "dataset_test_path = f'{root_path}/dataset/maestro-midi{AMOUNT}/test'\n",
    "\n",
    "# read pkl file\n",
    "dataset = pkl.load(open('dataset.pkl', 'rb'))\n",
    "train_time_diff_min = dataset['train_time_diff_min']\n",
    "train_time_diff_max = dataset['train_time_diff_max']\n",
    "note_num_min = dataset['note_num_min']\n",
    "note_num_max = dataset['note_num_max']\n",
    "length_min = dataset['length_min']\n",
    "length_max = dataset['length_max']\n",
    "velocity_min = dataset['velocity_min']\n",
    "velocity_max = dataset['velocity_max']\n",
    "dataset = None\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "extension = 'csv'\n",
    "os.chdir(dataset_test_path)\n",
    "test_csv_filenames = glob.glob('*.{}'.format(extension))\n",
    "csv_files_test = []\n",
    "for filename in test_csv_filenames:\n",
    "    df = pd.read_csv(filename, index_col=None, header=0)\n",
    "    csv_files_test.append(df)\n",
    "\n",
    "os.chdir(current_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_train = ['time_diff', 'note_num', 'length']\n",
    "columns_label = ['velocity']\n",
    "\n",
    "def divide_list(l, n, overlapping_window=0):\n",
    "    for i in range(0, len(l) - n + 1, n - overlapping_window):\n",
    "        yield l[i:i + n]\n",
    "    if len(l) % n != 0 and len(l) % n < n:\n",
    "        yield l[-(len(l) % n):]\n",
    "\n",
    "SAMPLE_LENGTH = 4\n",
    "def pad_data(data, feature_num):\n",
    "    if (len(data[-1]) != SAMPLE_LENGTH):\n",
    "        # print(f'Length of last array: {len(data[-1])}')\n",
    "        last_array = data.pop()\n",
    "        # print(f'before padding: {last_array}')\n",
    "        zero_array = np.zeros((SAMPLE_LENGTH - len(last_array), feature_num), dtype=np.float32)\n",
    "        last_array = np.concatenate((last_array, zero_array))\n",
    "        # print(f'after padding: {last_array}')\n",
    "        data.append(last_array)\n",
    "        # print(f'Length of last array (after padding): {len(data[-1])}')\n",
    "    return data\n",
    "\n",
    "def make_dataset(csv_data, columns_train, columns_label):\n",
    "    dataset_entire_input = np.empty((0, SAMPLE_LENGTH, 3), dtype=np.float32)\n",
    "    dataset_entire_label = np.empty((0, SAMPLE_LENGTH, 1), dtype=np.float32)\n",
    "\n",
    "    data_input_raw = np.array(csv_data[columns_train], dtype=np.float32)\n",
    "    data_label_raw = np.array(csv_data[columns_label], dtype=np.float32)\n",
    "\n",
    "    # normalize only the time difference\n",
    "    data_input_raw[:, 0] = (data_input_raw[:, 0] - train_time_diff_min) / (train_time_diff_max - train_time_diff_min)\n",
    "    # normalize only the note number\n",
    "    data_input_raw[:, 1] = (data_input_raw[:, 1] - note_num_min) / (note_num_max - note_num_min)\n",
    "    # normalize only the length\n",
    "    data_input_raw[:, 2] = (data_input_raw[:, 2] - length_min) / (length_max - length_min)\n",
    "    # normalize only the velocity\n",
    "    data_label_raw[:, 0] = (data_label_raw[:, 0] - velocity_min) / (velocity_max - velocity_min)\n",
    "\n",
    "    dataset_input = list(divide_list(data_input_raw, SAMPLE_LENGTH))\n",
    "    dataset_input = pad_data(dataset_input, 3)\n",
    "    dataset_input = np.array(dataset_input, dtype=np.float32)\n",
    "    dataset_entire_input = np.vstack((dataset_entire_input, dataset_input))\n",
    "\n",
    "    dataset_label = list(divide_list(data_label_raw, SAMPLE_LENGTH))\n",
    "    dataset_label = pad_data(dataset_label, 1)\n",
    "    dataset_label = np.array(dataset_label, dtype=np.float32)\n",
    "    dataset_entire_label = np.vstack((dataset_entire_label, dataset_label))    \n",
    "    \n",
    "    return dataset_entire_input, dataset_entire_label\n",
    "\n",
    "test_csv_file = csv_files_test[0]\n",
    "dataset_input, dataset_label = make_dataset(test_csv_file, columns_train, columns_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.losses import mse, cosine_similarity\n",
    "def make_mse_cosine_loss(alpha):\n",
    "    def mse_cosine_loss(y_true, y_pred):\n",
    "        # y_pred = tf.clip_by_value(y_pred, clip_value_min=0, clip_value_max=127)\n",
    "        return alpha * (1 * cosine_similarity(y_true, y_pred)) + (1 - alpha) * mse(y_true, y_pred)\n",
    "    return mse_cosine_loss\n",
    "ALPHA = 0.15\n",
    "mse_cosine_loss = make_mse_cosine_loss(ALPHA)\n",
    "\n",
    "def clipped_loss(y_true, y_pred):\n",
    "    y_pred = tf.clip_by_value(y_pred, clip_value_min=0, clip_value_max=127)\n",
    "    loss = tf.losses.mean_squared_error(y_true, y_pred)\n",
    "    return loss\n",
    "\n",
    "# model = keras.models.load_model('model.h5', custom_objects={'clipped_loss': clipped_loss})\n",
    "model = keras.models.load_model('model.h5', custom_objects={'mse_cosine_loss': mse_cosine_loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test_result = model.predict(dataset_input)\n",
    "print(dataset_test_result.shape, dataset_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = dataset_test_result.reshape(-1) * velocity_max\n",
    "true = dataset_label.reshape(-1) * velocity_max\n",
    "\n",
    "result = result.round()\n",
    "true = true\n",
    "\n",
    "# result = dataset_test_result.reshape(-1).astype(int)\n",
    "# np.clip(result, 0, 127, out=result)\n",
    "# true = dataset_label.reshape(-1).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 3))\n",
    "plt.plot(result[:500], label='result', linestyle='--', )\n",
    "plt.plot(true[:500], label='true', linestyle='-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get MAE between result_augmented and true\n",
    "mae = np.mean(np.abs(result - true))\n",
    "print(mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get difference between result and true\n",
    "diff = true - result\n",
    "\n",
    "# plot distribution of diff\n",
    "plt.figure(figsize=(10, 3))\n",
    "plt.hist(diff, bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "# Get approximated normal distribution of diff\n",
    "mean = np.mean(diff)\n",
    "std = np.std(diff)\n",
    "print(mean, std)\n",
    "\n",
    "# Make approximated normal distribution of diff\n",
    "x = np.linspace(mean - 3 * std, mean + 3 * std, 100)\n",
    "pdf = stats.norm.pdf(x, mean, std)\n",
    "\n",
    "# Plot approximated normal distribution of diff\n",
    "plt.plot(x, pdf, label='Approximate Normal Distribution')\n",
    "plt.hist(diff, bins=30, density=True, alpha=0.5, label='Original Distribution')\n",
    "plt.legend()\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('Probability Density')\n",
    "plt.title('Approximation of Normal Distribution')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_numbers = np.random.normal(mean, std * 0.2, size=len(result))\n",
    "result_augmented = result + random_numbers\n",
    "# clip the result_augmented\n",
    "result_augmented = np.clip(result_augmented, 0, 127)\n",
    "\n",
    "# plot the result_augmented and true values\n",
    "plt.figure(figsize=(20, 3))\n",
    "plt.plot(result_augmented[:500], label='result', linestyle='--', )\n",
    "plt.plot(true[:500], label='true', linestyle='-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get MAE between result_augmented and true\n",
    "mae = np.mean(np.abs(result_augmented - true))\n",
    "print(mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing data for csv\n",
    "def generate_csv(csv_file, filename, columns_input, result):\n",
    "    data_demo_input = np.array(csv_file[columns_input], dtype=int)\n",
    "    # Get the length of data_demo_input and result_augmented\n",
    "    length_data_demo = len(data_demo_input[:, 0])\n",
    "    length_result_demo = len(result.reshape(-1))\n",
    "    # print(length_data_demo, length_result_demo)\n",
    "    if (length_data_demo < length_result_demo):\n",
    "        result = result[:length_data_demo]\n",
    "\n",
    "    data_demo_velocity = np.array(np.round(result), dtype=int)\n",
    "\n",
    "    dataframe = pd.DataFrame({'time': data_demo_input[:, 0], \n",
    "                            'time_diff': data_demo_input[:, 1], \n",
    "                            'note_num': data_demo_input[:, 2], \n",
    "                            'length': data_demo_input[:, 3], \n",
    "                            'velocity': data_demo_velocity})\n",
    "    dataframe.to_csv(filename, index=False)\n",
    "\n",
    "columns_full_input = ['time', 'time_diff', 'note_num', 'length']\n",
    "generate_csv(test_csv_file, 'result.csv', columns_full_input, result)\n",
    "generate_csv(test_csv_file, 'result_augmented.csv', columns_full_input, result_augmented)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from subprocess import Popen, PIPE\n",
    "import os\n",
    "\n",
    "## util/csv2midi ??_predicted.csv ??.midi\n",
    "def csv2midi(csv_filename, midi_original_filename):\n",
    "    csv2midi_filename = '../util/csv2midi'\n",
    "\n",
    "    process = Popen([csv2midi_filename, csv_filename, midi_original_filename], stdout=PIPE, universal_newlines=True)\n",
    "    (output, err) = process.communicate()\n",
    "    exit_code = process.wait()\n",
    "\n",
    "    print(output)\n",
    "    print(f'csv2midi exit code: {exit_code}')\n",
    "\n",
    "original_midi_filename = f'midi.midi'\n",
    "result_csv_filename = 'result.csv'\n",
    "csv2midi(result_csv_filename, original_midi_filename)\n",
    "os.rename('midi_predicted.midi', 'midi_predicted_vanilla.midi')\n",
    "\n",
    "result_csv_filename = 'result_augmented.csv'\n",
    "csv2midi(result_csv_filename, original_midi_filename)\n",
    "os.rename('midi_predicted.midi', 'midi_predicted_augmented.midi')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
