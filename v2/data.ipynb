{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import spearmanr\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle as pkl\n",
    "\n",
    "import os, glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AMOUNT = '-small'\n",
    "# AMOUNT = '-medium'\n",
    "AMOUNT = '' # entire dataset\n",
    "\n",
    "root_path = '/home/tikim/code/midi-velocity-infer'\n",
    "dataset_train_path = f'{root_path}/dataset/maestro-midi{AMOUNT}/train'\n",
    "dataset_val_path = f'{root_path}/dataset/maestro-midi{AMOUNT}/validation'\n",
    "dataset_test_path = f'{root_path}/dataset/maestro-midi{AMOUNT}/test'\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "extension = 'csv'\n",
    "os.chdir(dataset_train_path)\n",
    "train_csv_filenames = glob.glob('*.{}'.format(extension))\n",
    "csv_files_train = []\n",
    "for filename in train_csv_filenames:\n",
    "    df = pd.read_csv(filename, index_col=None, header=0)\n",
    "    csv_files_train.append(df)\n",
    "dataset_entire_train = pd.concat(csv_files_train, axis=0, ignore_index=True)\n",
    "\n",
    "os.chdir(dataset_test_path)\n",
    "test_csv_filenames = glob.glob('*.{}'.format(extension))\n",
    "csv_files_test = []\n",
    "for filename in test_csv_filenames:\n",
    "    df = pd.read_csv(filename, index_col=None, header=0)\n",
    "    csv_files_test.append(df)\n",
    "    \n",
    "os.chdir(dataset_val_path)\n",
    "val_csv_filenames = glob.glob('*.{}'.format(extension))\n",
    "csv_files_val = []\n",
    "for filename in val_csv_filenames:\n",
    "    df = pd.read_csv(filename, index_col=None, header=0)\n",
    "    csv_files_val.append(df)\n",
    "    \n",
    "os.chdir(current_dir)\n",
    "\n",
    "# df = pd.read_csv('midi.csv', index_col=None, header=0)\n",
    "\n",
    "columns_train = ['time_diff', 'note_num', 'length']\n",
    "columns_label = ['velocity']\n",
    "\n",
    "dataset_entire_train = np.array(dataset_entire_train[columns_train], dtype=np.float32)\n",
    "\n",
    "train_time_diff_min = np.min(dataset_entire_train[:, 0])\n",
    "train_time_diff_max = np.max(dataset_entire_train[:, 0])\n",
    "\n",
    "length_min = np.min(dataset_entire_train[:, 2])\n",
    "length_max = np.max(dataset_entire_train[:, 2])\n",
    "\n",
    "note_num_min = 0\n",
    "note_num_max = 127\n",
    "\n",
    "velocity_min = 0\n",
    "velocity_max = 127\n",
    "\n",
    "dataset_entire_train = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_list(l, n, overlapping_window=0):\n",
    "    for i in range(0, len(l) - n + 1, n - overlapping_window):\n",
    "        yield l[i:i + n]\n",
    "    if len(l) % n != 0 and len(l) % n < n:\n",
    "        yield l[-(len(l) % n):]\n",
    "                \n",
    "SAMPLE_LENGTH = 4\n",
    "def pad_data(data, feature_num):\n",
    "            if (len(data[-1]) != SAMPLE_LENGTH):\n",
    "                # print(f'Length of last array: {len(data[-1])}')\n",
    "                last_array = data.pop()\n",
    "                # print(f'before padding: {last_array}')\n",
    "                zero_array = np.zeros((SAMPLE_LENGTH - len(last_array), feature_num), dtype=np.float32)\n",
    "                last_array = np.concatenate((last_array, zero_array))\n",
    "                # print(f'after padding: {last_array}')\n",
    "                data.append(last_array)\n",
    "                # print(f'Length of last array (after padding): {len(data[-1])}')\n",
    "            return data\n",
    "\n",
    "def make_dataset(csv_files, columns_train, columns_label):\n",
    "    dataset_entire_input = np.empty((0, SAMPLE_LENGTH, 3), dtype=np.float32)\n",
    "    dataset_entire_label = np.empty((0, SAMPLE_LENGTH, 1), dtype=np.float32)\n",
    "    \n",
    "    for df in csv_files:\n",
    "        data_input_raw = np.array(df[columns_train], dtype=np.float32)\n",
    "        data_label_raw = np.array(df[columns_label], dtype=np.float32)\n",
    "        \n",
    "        # normalize only the time difference\n",
    "        data_input_raw[:, 0] = (data_input_raw[:, 0] - train_time_diff_min) / (train_time_diff_max - train_time_diff_min)\n",
    "        # normalize only the note number\n",
    "        data_input_raw[:, 1] = (data_input_raw[:, 1] - note_num_min) / (note_num_max - note_num_min)\n",
    "        # normalize only the length\n",
    "        data_input_raw[:, 2] = (data_input_raw[:, 2] - length_min) / (length_max - length_min)\n",
    "        # normalize only the velocity\n",
    "        data_label_raw[:, 0] = (data_label_raw[:, 0] - velocity_min) / (velocity_max - velocity_min)\n",
    "\n",
    "        data_input_raw2 = list(divide_list(data_input_raw, SAMPLE_LENGTH, SAMPLE_LENGTH - 1))\n",
    "        data_input_raw2 = pad_data(data_input_raw2, 3)\n",
    "        data_input = np.array(data_input_raw2, dtype=np.float32)\n",
    "        dataset_entire_input = np.vstack((dataset_entire_input, data_input))\n",
    "\n",
    "        data_label_raw2 = list(divide_list(data_label_raw, SAMPLE_LENGTH, SAMPLE_LENGTH - 1))\n",
    "        data_label_raw2 = pad_data(data_label_raw2, 1)\n",
    "        data_label = np.array(data_label_raw2, dtype=np.float32)\n",
    "        dataset_entire_label = np.vstack((dataset_entire_label, data_label))\n",
    "    \n",
    "    return dataset_entire_input, dataset_entire_label\n",
    "\n",
    "dataset_train_input, dataset_train_label = make_dataset(csv_files_train, columns_train, columns_label)\n",
    "dataset_val_input, dataset_val_label = make_dataset(csv_files_val, columns_train, columns_label)\n",
    "dataset_test_input, dataset_test_label = make_dataset(csv_files_test, columns_train, columns_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl.dump({'dataset_train_input': dataset_train_input, 'dataset_train_label': dataset_train_label,\n",
    "          'dataset_val_input': dataset_val_input, 'dataset_val_label': dataset_val_label,\n",
    "          'dataset_test_input': dataset_test_input, 'dataset_test_label': dataset_test_label,\n",
    "          'train_time_diff_min': train_time_diff_min, 'train_time_diff_max': train_time_diff_max, \n",
    "          'note_num_min': note_num_min, 'note_num_max': note_num_max, \n",
    "          'length_min': length_min, 'length_max': length_max,\n",
    "          'velocity_min': velocity_min, 'velocity_max': velocity_max}, open('dataset.pkl', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
