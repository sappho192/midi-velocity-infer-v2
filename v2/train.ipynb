{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle as pkl\n",
    "import keras\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import LSTM, Dense, RepeatVector, TimeDistributed, Bidirectional, Input, BatchNormalization, \\\n",
    "    multiply, concatenate, Flatten, Activation, dot, LeakyReLU\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import plot_model\n",
    "from keras.callbacks import EarlyStopping\n",
    "import tensorflow as tf\n",
    "\n",
    "import pydot as pyd\n",
    "from keras.utils.vis_utils import plot_model, model_to_dot\n",
    "keras.utils.vis_utils.pydot = pyd\n",
    "\n",
    "import wandb\n",
    "from wandb.keras import WandbMetricsLogger, WandbModelCheckpoint\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "  tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pkl.load(open('dataset.pkl', 'rb'))\n",
    "\n",
    "dataset_train_input = data['dataset_train_input']\n",
    "dataset_train_label = data['dataset_train_label']\n",
    "dataset_val_input = data['dataset_val_input']\n",
    "dataset_val_label = data['dataset_val_label']\n",
    "dataset_test_input = data['dataset_test_input']\n",
    "dataset_test_label = data['dataset_test_label']\n",
    "\n",
    "velocity_min = data['velocity_min']\n",
    "velocity_max = data['velocity_max']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_train = Input(shape=(dataset_train_input.shape[1], dataset_train_input.shape[2]))\n",
    "label_train = Input(shape=(dataset_train_label.shape[1], dataset_train_label.shape[2]))\n",
    "\n",
    "print(input_train)\n",
    "print(label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_HIDDEN = 4\n",
    "\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"midi-velocity-infer-v2\",\n",
    "\n",
    "    # track hyperparameters and run metadata with wandb.config\n",
    "    config={\n",
    "        \"n_hidden\": N_HIDDEN,\n",
    "        \"activation_1\": \"LeakyRelu\",\n",
    "        \"dropout\": 0.2,\n",
    "        \"optimizer\": \"adam\",\n",
    "        \"loss\": \"mse_cosine_loss\",\n",
    "        \"metric\": \"mae\",\n",
    "        \"epoch\": 20,\n",
    "    }\n",
    ")\n",
    "config = wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_last_h1, encoder_last_h2, encoder_last_c = LSTM(\n",
    "    config.n_hidden, activation=LeakyReLU(),\n",
    "    input_shape=(dataset_train_input.shape[1], dataset_train_input.shape[2]), \n",
    "    return_sequences=False, return_state=True\n",
    ")(input_train)\n",
    "print(encoder_last_h1)\n",
    "print(encoder_last_h2)\n",
    "print(encoder_last_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_last_h1 = BatchNormalization(momentum=0.9)(encoder_last_h1) \n",
    "print(encoder_last_h1)\n",
    "encoder_last_c = BatchNormalization(momentum=0.9)(encoder_last_c) \n",
    "print(encoder_last_c)\n",
    "decoder = RepeatVector(dataset_train_input.shape[1])(encoder_last_h1)\n",
    "print(decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = LSTM(\n",
    "    N_HIDDEN, activation=LeakyReLU(), dropout=config.dropout,\n",
    "    return_sequences=True, return_state=False\n",
    ")(decoder, initial_state=[encoder_last_h1, encoder_last_c])\n",
    "print(decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = TimeDistributed(Dense(dataset_train_label.shape[2]))(decoder)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=input_train, outputs=out)\n",
    "opt = Adam(lr=0.0001)\n",
    "\n",
    "from keras.losses import mse, cosine_similarity\n",
    "def make_mse_cosine_loss(alpha):\n",
    "    def mse_cosine_loss(y_true, y_pred):\n",
    "        # y_pred = tf.clip_by_value(y_pred, clip_value_min=0, clip_value_max=127)\n",
    "        return alpha * (1 * cosine_similarity(y_true, y_pred)) + (1 - alpha) * mse(y_true, y_pred)\n",
    "    return mse_cosine_loss\n",
    "ALPHA = 0.10\n",
    "mse_cosine_loss = make_mse_cosine_loss(ALPHA)\n",
    "\n",
    "def clipped_loss(y_true, y_pred):\n",
    "    y_pred = tf.clip_by_value(y_pred, clip_value_min=0, clip_value_max=127)\n",
    "    loss = tf.losses.mean_squared_error(y_true, y_pred)\n",
    "    return loss\n",
    "\n",
    "model.compile(loss=mse_cosine_loss, optimizer=opt, metrics=['mae'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "current_time = datetime.now().strftime('%Y-%m-%d_%H-%M_%S')\n",
    "\n",
    "import os\n",
    "os.makedirs(f'saved_models', exist_ok=True)\n",
    "\n",
    "import logging\n",
    "tf.get_logger().setLevel(logging.ERROR) # TODO: comment if you need to debug\n",
    "\n",
    "# es = EarlyStopping(monitor='val_loss', patience=10)\n",
    "# history = model.fit(dataset_train_input, dataset_train_label, epochs=epoch, validation_data=(dataset_val_input, dataset_val_label), callbacks=[es])\n",
    "history = model.fit(dataset_train_input, dataset_train_label, epochs=config.epoch, validation_data=(dataset_val_input, dataset_val_label)\n",
    "                    , callbacks=[WandbMetricsLogger(log_freq=5), WandbModelCheckpoint(\"models\")])\n",
    "wandb.finish()\n",
    "model.save(f'mvi-v2-{current_time}-h{config.n_hidden}-{config.loss}-no_attention.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mae = history.history['mae']\n",
    "valid_mae = history.history['val_mae']\n",
    "\n",
    "plt.plot(train_mae, label='train mae'), \n",
    "plt.plot(valid_mae, label='validation mae')\n",
    "plt.ylabel('mae')\n",
    "plt.xlabel('epoch')\n",
    "plt.title('train vs. validation accuracy (mae)')\n",
    "plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.15), fancybox=True, shadow=False, ncol=2)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
