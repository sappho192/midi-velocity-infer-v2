{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import spearmanr\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle as pkl\n",
    "\n",
    "import os, glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'MAESTRO'\n",
    "# dataset_name = 'GiantMIDIPiano'\n",
    "# dataset_name = 'chien2021'\n",
    "\n",
    "# FLOAT_DTYPE = np.float64\n",
    "FLOAT_DTYPE = np.float32\n",
    "\n",
    "AMOUNT = ''\n",
    "root_path = ''\n",
    "dataset_train_path = ''\n",
    "dataset_val_path = ''\n",
    "dataset_test_path = ''\n",
    "if dataset_name == 'MAESTRO':\n",
    "    # AMOUNT = '-small'\n",
    "    # AMOUNT = '-medium'\n",
    "    AMOUNT = '' # entire dataset\n",
    "\n",
    "    root_path = '/home/tikim/code/midi-velocity-infer'\n",
    "    dataset_train_path = f'{root_path}/dataset/maestro-midi{AMOUNT}/train'\n",
    "    dataset_val_path = f'{root_path}/dataset/maestro-midi{AMOUNT}/validation'\n",
    "    dataset_test_path = f'{root_path}/dataset/maestro-midi{AMOUNT}/test'\n",
    "elif dataset_name == 'GiantMIDIPiano':\n",
    "    root_path = '/home/tikim/dataset/giantmidi'\n",
    "    dataset_train_path = f'{root_path}/dataset/train'\n",
    "    dataset_val_path = f'{root_path}/dataset/validation'\n",
    "    dataset_test_path = f'{root_path}/dataset/test'\n",
    "elif dataset_name == 'chien2021':\n",
    "    root_path = '/home/tikim/dataset/chien2021'\n",
    "    dataset_train_path = f'{root_path}/dataset/train'\n",
    "    dataset_test_path = f'{root_path}/dataset/test'\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "extension = 'csv'\n",
    "os.chdir(dataset_train_path)\n",
    "train_csv_filenames = glob.glob('*.{}'.format(extension))\n",
    "csv_files_train = []\n",
    "for filename in train_csv_filenames:\n",
    "    df = pd.read_csv(filename, index_col=None, header=0)\n",
    "    if len(df) == 0:\n",
    "        print(f'Dataframe is empty! {filename} skipping...')\n",
    "        continue\n",
    "    csv_files_train.append(df)\n",
    "dataset_entire_train = pd.concat(csv_files_train, axis=0, ignore_index=True)\n",
    "\n",
    "os.chdir(dataset_test_path)\n",
    "test_csv_filenames = glob.glob('*.{}'.format(extension))\n",
    "csv_files_test = []\n",
    "for filename in test_csv_filenames:\n",
    "    if len(df) == 0:\n",
    "        print(f'Dataframe is empty! {filename} skipping...')\n",
    "        continue\n",
    "    df = pd.read_csv(filename, index_col=None, header=0)\n",
    "    csv_files_test.append(df)\n",
    "\n",
    "if (dataset_name != 'chien2021'):\n",
    "    os.chdir(dataset_val_path)\n",
    "    val_csv_filenames = glob.glob('*.{}'.format(extension))\n",
    "    csv_files_val = []\n",
    "    for filename in val_csv_filenames:\n",
    "        if len(df) == 0:\n",
    "            print(f'Dataframe is empty! {filename} skipping...')\n",
    "            continue\n",
    "        df = pd.read_csv(filename, index_col=None, header=0)\n",
    "        csv_files_val.append(df)\n",
    "    \n",
    "os.chdir(current_dir)\n",
    "\n",
    "# df = pd.read_csv('midi.csv', index_col=None, header=0)\n",
    "\n",
    "columns_train = ['time_diff', 'note_num', 'length', 'note_num_diff', 'low_octave'] #, 'time']\n",
    "columns_label = ['velocity']\n",
    "\n",
    "dataset_entire_train = np.array(dataset_entire_train[columns_train], dtype=FLOAT_DTYPE)\n",
    "\n",
    "train_time_diff_min = np.min(dataset_entire_train[:, 0])\n",
    "train_time_diff_max = np.max(dataset_entire_train[:, 0])\n",
    "\n",
    "length_min = np.min(dataset_entire_train[:, 2])\n",
    "length_max = np.max(dataset_entire_train[:, 2])\n",
    "\n",
    "note_num_min = 0\n",
    "note_num_max = 127\n",
    "\n",
    "note_num_diff_min = np.min(dataset_entire_train[:, 3])\n",
    "note_num_diff_max = np.max(dataset_entire_train[:, 3])\n",
    "\n",
    "# time_min = np.min(dataset_entire_train[:, 5])\n",
    "# time_max = np.max(dataset_entire_train[:, 5])\n",
    "\n",
    "velocity_min = 0\n",
    "velocity_max = 127\n",
    "\n",
    "dataset_entire_train = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_list(l, n, overlapping_window=0):\n",
    "    for i in range(0, len(l) - n + 1, n - overlapping_window):\n",
    "        yield l[i:i + n]\n",
    "    if len(l) % n != 0 and len(l) % n < n:\n",
    "        yield l[-(len(l) % n):]\n",
    "                \n",
    "SAMPLE_LENGTH = 8\n",
    "FEATURE_NUM = 5\n",
    "def pad_data(data, feature_num):\n",
    "    # print(f'Length of data: {len(data)}')\n",
    "    if len(data[-1]) != SAMPLE_LENGTH:\n",
    "        # print(f'Length of last array: {len(data[-1])}')\n",
    "        last_array = data.pop()\n",
    "        # print(f'before padding: {last_array}')\n",
    "        zero_array = np.zeros((SAMPLE_LENGTH - len(last_array), feature_num), dtype=FLOAT_DTYPE)\n",
    "        last_array = np.concatenate((last_array, zero_array))\n",
    "        # print(f'after padding: {last_array}')\n",
    "        data.append(last_array)\n",
    "        # print(f'Length of last array (after padding): {len(data[-1])}')\n",
    "    return data\n",
    "\n",
    "def make_dataset(csv_files, columns_train, columns_label):\n",
    "    dataset_entire_input = np.empty((0, SAMPLE_LENGTH, FEATURE_NUM), dtype=FLOAT_DTYPE)\n",
    "    dataset_entire_label = np.empty((0, SAMPLE_LENGTH, 1), dtype=FLOAT_DTYPE)\n",
    "    \n",
    "    for df in csv_files:\n",
    "        data_input_raw = np.array(df[columns_train], dtype=FLOAT_DTYPE)\n",
    "        data_label_raw = np.array(df[columns_label], dtype=FLOAT_DTYPE)\n",
    "        \n",
    "        # normalize the time difference\n",
    "        data_input_raw[:, 0] = (data_input_raw[:, 0] - train_time_diff_min) / (train_time_diff_max - train_time_diff_min)\n",
    "        # normalize the note number\n",
    "        data_input_raw[:, 1] = (data_input_raw[:, 1] - note_num_min) / (note_num_max - note_num_min)\n",
    "        # normalize the length\n",
    "        data_input_raw[:, 2] = (data_input_raw[:, 2] - length_min) / (length_max - length_min)\n",
    "        # normalize the note number difference\n",
    "        data_input_raw[:, 3] = (data_input_raw[:, 3] - note_num_diff_min) / (note_num_diff_max - note_num_diff_min)\n",
    "        # you don't have to normalize the low octave\n",
    "        # normalize the time\n",
    "        # data_input_raw[:, 5] = (data_input_raw[:, 5] - time_min) / (time_max - time_min)\n",
    "        \n",
    "        # normalize the velocity\n",
    "        data_label_raw[:, 0] = (data_label_raw[:, 0] - velocity_min) / (velocity_max - velocity_min)\n",
    "\n",
    "        data_input_raw2 = list(divide_list(data_input_raw, SAMPLE_LENGTH, SAMPLE_LENGTH - 1))\n",
    "        data_input_raw2 = pad_data(data_input_raw2, FEATURE_NUM)\n",
    "        data_input = np.array(data_input_raw2, dtype=FLOAT_DTYPE)\n",
    "        dataset_entire_input = np.vstack((dataset_entire_input, data_input))\n",
    "\n",
    "        data_label_raw2 = list(divide_list(data_label_raw, SAMPLE_LENGTH, SAMPLE_LENGTH - 1))\n",
    "        data_label_raw2 = pad_data(data_label_raw2, 1)\n",
    "        data_label = np.array(data_label_raw2, dtype=FLOAT_DTYPE)\n",
    "        dataset_entire_label = np.vstack((dataset_entire_label, data_label))\n",
    "    \n",
    "    return dataset_entire_input, dataset_entire_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (dataset_name == 'MAESTRO' or dataset_name == 'GiantMIDIPiano'):\n",
    "    dataset_train_input, dataset_train_label = make_dataset(csv_files_train, columns_train, columns_label)\n",
    "    dataset_val_input, dataset_val_label = make_dataset(csv_files_val, columns_train, columns_label)\n",
    "    dataset_test_input, dataset_test_label = make_dataset(csv_files_test, columns_train, columns_label)\n",
    "elif (dataset_name == 'chien2021'):\n",
    "    dataset_train_input, dataset_train_label = make_dataset(csv_files_train, columns_train, columns_label)\n",
    "    dataset_test_input, dataset_test_label = make_dataset(csv_files_test, columns_train, columns_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_filename_short = ''\n",
    "dataset_filename = ''\n",
    "if dataset_name == 'MAESTRO':\n",
    "    if(FLOAT_DTYPE == np.float32):\n",
    "        dataset_filename_short = f'dataset32-{dataset_name}-len{SAMPLE_LENGTH}{AMOUNT}'\n",
    "    elif (FLOAT_DTYPE == np.float64):\n",
    "        dataset_filename_short = f'dataset64-{dataset_name}-len{SAMPLE_LENGTH}{AMOUNT}'\n",
    "    else:\n",
    "        dataset_filename_short = 'dataset'\n",
    "elif dataset_name == 'GiantMIDIPiano':\n",
    "    if (FLOAT_DTYPE == np.float32):\n",
    "        dataset_filename_short = f'dataset32-{dataset_name}-len{SAMPLE_LENGTH}'\n",
    "    elif (FLOAT_DTYPE == np.float64):\n",
    "        dataset_filename_short = f'dataset64-{dataset_name}-len{SAMPLE_LENGTH}'\n",
    "    else:\n",
    "        dataset_filename_short = 'dataset'\n",
    "elif (dataset_name == 'chien2021'):\n",
    "    if (FLOAT_DTYPE == np.float32):\n",
    "        dataset_filename_short = f'dataset32-{dataset_name}-len{SAMPLE_LENGTH}'\n",
    "    elif (FLOAT_DTYPE == np.float64):\n",
    "        dataset_filename_short = f'dataset64-{dataset_name}-len{SAMPLE_LENGTH}'\n",
    "    else:\n",
    "        dataset_filename_short = 'dataset'\n",
    "dataset_filename = f'{dataset_filename_short}.pkl'\n",
    "\n",
    "if (dataset_name == 'MAESTRO' or dataset_name == 'GiantMIDIPiano'):\n",
    "    pkl.dump({'dataset_train_input': dataset_train_input, 'dataset_train_label': dataset_train_label,\n",
    "            'dataset_val_input': dataset_val_input, 'dataset_val_label': dataset_val_label,\n",
    "            'dataset_test_input': dataset_test_input, 'dataset_test_label': dataset_test_label,\n",
    "            'train_time_diff_min': train_time_diff_min, 'train_time_diff_max': train_time_diff_max, \n",
    "            'note_num_min': note_num_min, 'note_num_max': note_num_max, \n",
    "            'note_num_diff_min': note_num_diff_min, 'note_num_diff_max': note_num_diff_max,\n",
    "            'length_min': length_min, 'length_max': length_max,\n",
    "            # 'time_min': time_min, 'time_max': time_max,\n",
    "            'velocity_min': velocity_min, 'velocity_max': velocity_max}, open(dataset_filename, 'wb'))\n",
    "elif (dataset_name == 'chien2021'):\n",
    "    pkl.dump({'dataset_train_input': dataset_train_input, 'dataset_train_label': dataset_train_label,\n",
    "            'dataset_test_input': dataset_test_input, 'dataset_test_label': dataset_test_label,\n",
    "            'train_time_diff_min': train_time_diff_min, 'train_time_diff_max': train_time_diff_max,\n",
    "            'note_num_min': note_num_min, 'note_num_max': note_num_max,\n",
    "            'note_num_diff_min': note_num_diff_min, 'note_num_diff_max': note_num_diff_max,\n",
    "            'length_min': length_min, 'length_max': length_max,\n",
    "            # 'time_min': time_min, 'time_max': time_max,\n",
    "            'velocity_min': velocity_min, 'velocity_max': velocity_max}, open(dataset_filename, 'wb'))\n",
    "\n",
    "import json\n",
    "# save metadata to json. Convert float32 to float\n",
    "metadata = {'train_time_diff_min': train_time_diff_min, 'train_time_diff_max': train_time_diff_max,\n",
    "            'note_num_min': note_num_min, 'note_num_max': note_num_max,\n",
    "            'note_num_diff_min': note_num_diff_min, 'note_num_diff_max': note_num_diff_max,\n",
    "            'length_min': length_min, 'length_max': length_max,\n",
    "            # 'time_min': time_min, 'time_max': time_max,\n",
    "            'velocity_min': velocity_min, 'velocity_max': velocity_max}\n",
    "with open(f'{dataset_filename_short}.json', 'w') as f:\n",
    "    json.dump(metadata, f, default=int)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
