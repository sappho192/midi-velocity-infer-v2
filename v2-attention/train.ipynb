{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle as pkl\n",
    "import keras\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import LSTM, Dense, RepeatVector, TimeDistributed, Bidirectional, Input, BatchNormalization, \\\n",
    "    multiply, concatenate, Flatten, Activation, dot, LeakyReLU, GRU\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import plot_model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "import pydot as pyd\n",
    "from keras.utils.vis_utils import plot_model, model_to_dot\n",
    "keras.utils.vis_utils.pydot = pyd\n",
    "\n",
    "import wandb\n",
    "from wandb.keras import WandbMetricsLogger, WandbModelCheckpoint\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "  tf.config.experimental.set_memory_growth(gpu, True)\n",
    "  \n",
    "# dataset_filename = 'dataset32.pkl'\n",
    "# if (dataset_filename == 'dataset64.pkl'):\n",
    "#   tf.keras.backend.set_floatx('float64')\n",
    "# else:\n",
    "#   tf.keras.backend.set_floatx('float32')\n",
    "\n",
    "dataset_name = \"MAESTRO\"\n",
    "# dataset_name = \"GiantMIDIPiano\"\n",
    "# dataset_name = \"chien2021\"\n",
    "\n",
    "N_HIDDEN = 4\n",
    "dataset_filename = ''\n",
    "if dataset_name == 'MAESTRO':\n",
    "  dataset_filename = f'dataset32-{dataset_name}-len{N_HIDDEN}.pkl'\n",
    "elif dataset_name == 'GiantMIDIPiano':\n",
    "  dataset_filename = f'dataset32-{dataset_name}-len{N_HIDDEN}.pkl'\n",
    "elif dataset_name == 'chien2021':\n",
    "  dataset_filename = f'dataset32-{dataset_name}-len{N_HIDDEN}.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pkl.load(open(dataset_filename, 'rb'))\n",
    "\n",
    "dataset_train_input = data['dataset_train_input']\n",
    "dataset_train_label = data['dataset_train_label']\n",
    "if (dataset_name != 'chien2021'):\n",
    "    dataset_val_input = data['dataset_val_input']\n",
    "    dataset_val_label = data['dataset_val_label']\n",
    "dataset_test_input = data['dataset_test_input']\n",
    "dataset_test_label = data['dataset_test_label']\n",
    "\n",
    "velocity_min = data['velocity_min']\n",
    "velocity_max = data['velocity_max']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_train = Input(shape=(dataset_train_input.shape[1], dataset_train_input.shape[2]))\n",
    "label_train = Input(shape=(dataset_train_label.shape[1], dataset_train_label.shape[2]))\n",
    "\n",
    "print(input_train)\n",
    "print(label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"midi-velocity-infer-v2-attention\",\n",
    "\n",
    "    # track hyperparameters and run metadata with wandb.config\n",
    "    config={\n",
    "        \"n_hidden\": N_HIDDEN,\n",
    "        \"activation_1\": \"LeakyRelu\",\n",
    "        \"dropout\": 0.2,\n",
    "        \"optimizer\": \"adam\",\n",
    "        \"loss\": \"mse_cosine_loss\",\n",
    "        \"loss_alpha\": 0.15,\n",
    "        \"batchnorm_momentum\": 0.60,\n",
    "        \"metric\": \"mae\",\n",
    "        \"epoch\": 5,\n",
    "        \"teacher_forcing\": False,\n",
    "        \"rnn_type\": \"LSTM\",\n",
    "        \"dataset_name\": dataset_name\n",
    "    }\n",
    ")\n",
    "config = wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.rnn_type == \"LSTM\":\n",
    "    encoder_stack_h, encoder_last_h, encoder_last_c = LSTM(\n",
    "        config.n_hidden, activation=LeakyReLU(),\n",
    "        input_shape=(dataset_train_input.shape[1], dataset_train_input.shape[2]), \n",
    "        return_sequences=True, return_state=True\n",
    "    )(input_train)\n",
    "    print(encoder_stack_h)\n",
    "    print(encoder_last_h)\n",
    "    print(encoder_last_c)\n",
    "elif config.rnn_type == \"GRU\":\n",
    "    encoder_stack_h, encoder_last_h = GRU(\n",
    "    config.n_hidden, activation=LeakyReLU(),\n",
    "    input_shape=(dataset_train_input.shape[1], dataset_train_input.shape[2]), \n",
    "    return_sequences=True, return_state=True\n",
    "    )(input_train)\n",
    "    print(encoder_stack_h)\n",
    "    print(encoder_last_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_last_h = BatchNormalization(momentum=config.batchnorm_momentum)(encoder_last_h) \n",
    "print(encoder_last_h)\n",
    "if config.rnn_type == \"LSTM\":\n",
    "    encoder_last_c = BatchNormalization(momentum=config.batchnorm_momentum)(encoder_last_c) \n",
    "    print(encoder_last_c)\n",
    "decoder_input = RepeatVector(dataset_train_input.shape[1])(encoder_last_h)\n",
    "print(decoder_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.rnn_type == \"LSTM\":\n",
    "    decoder_stack_h = LSTM(\n",
    "        N_HIDDEN, activation=LeakyReLU(), dropout=config.dropout,\n",
    "        return_sequences=True, return_state=False\n",
    "    )(decoder_input, initial_state=[encoder_last_h, encoder_last_c])\n",
    "elif config.rnn_type == \"GRU\":\n",
    "    decoder_stack_h = GRU(\n",
    "    N_HIDDEN, activation=LeakyReLU(), dropout=config.dropout,\n",
    "    return_sequences=True, return_state=False\n",
    "    )(decoder_input, initial_state=[encoder_last_h])\n",
    "\n",
    "print(decoder_stack_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention = dot([decoder_stack_h, encoder_stack_h], axes=[2, 2])\n",
    "attention = Activation('softmax')(attention)\n",
    "print(attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = dot([attention, encoder_stack_h], axes=[2, 1])\n",
    "context = BatchNormalization(momentum=config.batchnorm_momentum)(context)\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_combined_context = concatenate([context, decoder_stack_h])\n",
    "print(decoder_combined_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = TimeDistributed(Dense(dataset_train_label.shape[2]))(decoder_combined_context)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=input_train, outputs=out)\n",
    "# step_size = 176787\n",
    "lr_decay_alpha = 1.0\n",
    "cosine_decay_annealing_scheduler = tf.keras.optimizers.schedules.CosineDecayRestarts(\n",
    "    initial_learning_rate=0.0001, first_decay_steps=10000, alpha=0, t_mul=2, m_mul=0.9\n",
    ")\n",
    "\n",
    "if (dataset_name == 'chien2021'):\n",
    "    cosine_decay_scheduler = tf.keras.optimizers.schedules.CosineDecay(\n",
    "        # initial_learning_rate=0.0001, decay_steps=176787*config.epoch, alpha=0.001 # MAESTRO full\n",
    "        initial_learning_rate=0.0001, decay_steps=31219*config.epoch*lr_decay_alpha, alpha=0.001 # chien2021\n",
    "        # initial_learning_rate=0.0001, decay_steps=1889372, alpha=0.7 # GiantMIDIPiano, epoch4\n",
    "    )\n",
    "elif (dataset_name == 'MAESTRO'):\n",
    "    cosine_decay_scheduler = tf.keras.optimizers.schedules.CosineDecay(\n",
    "        initial_learning_rate=0.0001, decay_steps=176787*config.epoch*lr_decay_alpha, alpha=0.001 # MAESTRO full len4\n",
    "        # initial_learning_rate=0.0001, decay_steps=176670*config.epoch*lr_decay_alpha, alpha=0.001 # MAESTRO full len8\n",
    "        # initial_learning_rate=0.0001, decay_steps=176432*config.epoch*lr_decay_alpha, alpha=0.001 # MAESTRO full len16\n",
    "        # initial_learning_rate=0.0001, decay_steps=175952*config.epoch*lr_decay_alpha, alpha=0.001 # MAESTRO full len32\n",
    "    )\n",
    "elif (dataset_name == 'GiantMIDIPiano'):\n",
    "        cosine_decay_scheduler = tf.keras.optimizers.schedules.CosineDecay(\n",
    "        initial_learning_rate=0.0001, decay_steps=1889372, alpha=0.7 # GiantMIDIPiano, epoch4\n",
    "    )\n",
    "\n",
    "step_decay_scheduler = tf.keras.optimizers.schedules.PolynomialDecay(\n",
    "    initial_learning_rate=0.0001,\n",
    "    decay_steps=95400,\n",
    "    end_learning_rate=0.000001,\n",
    "    power = 1.0\n",
    ")\n",
    "\n",
    "# opt = Adam(learning_rate=cosine_decay_annealing_scheduler)\n",
    "opt = Adam(learning_rate=cosine_decay_scheduler)\n",
    "# opt = Adam(learning_rate=step_decay_scheduler)\n",
    "# opt = Adam(lr=0.0001, clip_norm=1.0, clipvalue=0.5)\n",
    "\n",
    "from keras.losses import mse, cosine_similarity, mae\n",
    "def make_mse_cosine_loss(alpha):\n",
    "    def mse_cosine_loss(y_true, y_pred):\n",
    "        # y_pred = tf.clip_by_value(y_pred, clip_value_min=0, clip_value_max=127)\n",
    "        return alpha * (1 * cosine_similarity(y_true, y_pred)) + (1 - alpha) * mse(y_true, y_pred)\n",
    "    return mse_cosine_loss\n",
    "ALPHA = config.loss_alpha\n",
    "mse_cosine_loss = make_mse_cosine_loss(ALPHA)\n",
    "\n",
    "def clipped_loss(y_true, y_pred):\n",
    "    y_pred = tf.clip_by_value(y_pred, clip_value_min=0, clip_value_max=127)\n",
    "    loss = tf.losses.mean_squared_error(y_true, y_pred)\n",
    "    return loss\n",
    "\n",
    "model.compile(loss=mse_cosine_loss, optimizer=opt, metrics=['mae'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "current_time = datetime.now().strftime('%Y-%m-%d_%H-%M_%S')\n",
    "print(f'Current time: {current_time}')\n",
    "\n",
    "import os\n",
    "os.makedirs(f'saved_models', exist_ok=True)\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "import logging\n",
    "tf.get_logger().setLevel(logging.ERROR) # TODO: comment if you need to debug\n",
    "\n",
    "# es = EarlyStopping(monitor='val_loss', patience=10)\n",
    "# history = model.fit(dataset_train_input, dataset_train_label, epochs=epoch, validation_data=(dataset_val_input, dataset_val_label), callbacks=[es])\n",
    "\n",
    "if (dataset_name == 'chien2021'):\n",
    "    history = model.fit(dataset_train_input, dataset_train_label, epochs=config.epoch, \n",
    "                        callbacks=[WandbMetricsLogger(log_freq=5), WandbModelCheckpoint(\"models\")])\n",
    "else:\n",
    "    history = model.fit(dataset_train_input, dataset_train_label, epochs=config.epoch, validation_data=(dataset_val_input, dataset_val_label)\n",
    "                        , callbacks=[WandbMetricsLogger(log_freq=5), WandbModelCheckpoint(\"models\")])\n",
    "model.save(f'saved_models/mvi-v2-{current_time}-h{config.n_hidden}-e{config.epoch}-{config.loss}-alpha{config.loss_alpha:.2f}-m{config.batchnorm_momentum:.2f}-{config.rnn_type}-luong_attention-{config.dataset_name}.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save(f'saved_models/mvi-v2-{current_time}-h{config.n_hidden}-e{config.epoch}-{config.loss}-alpha{config.loss_alpha:.2f}-{config.rnn_type}-luong_attention-{config.dataset_name}.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.alert(title='Training finished', text=f'loss: {history.history[\"loss\"][-1]:.4f}, mae: {history.history[\"mae\"][-1]:.4f}', level=wandb.AlertLevel.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (dataset_name == 'chien2021'):\n",
    "    train_mae = history.history['mae']\n",
    "    \n",
    "    plt.plot(train_mae, label='train mae')\n",
    "    plt.ylabel('MAE')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.title('MAE(train)')\n",
    "    plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.15), fancybox=True, shadow=False, ncol=2)\n",
    "    plt.show()\n",
    "else:\n",
    "    train_mae = history.history['mae']\n",
    "    valid_mae = history.history['val_mae']\n",
    "\n",
    "    plt.plot(train_mae, label='train mae'), \n",
    "    plt.plot(valid_mae, label='validation mae')\n",
    "    plt.ylabel('mae')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.title('train vs. validation accuracy (mae)')\n",
    "    plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.15), fancybox=True, shadow=False, ncol=2)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
