{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.utils import plot_model\n",
    "\n",
    "import pydot as pyd\n",
    "from keras.utils.vis_utils import plot_model, model_to_dot\n",
    "keras.utils.vis_utils.pydot = pyd\n",
    "import tensorflow as tf\n",
    "\n",
    "import os, glob\n",
    "import pandas as pd\n",
    "import platform\n",
    "\n",
    "CURRENT_OS = platform.system()\n",
    "print(CURRENT_OS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.losses import mse, cosine_similarity\n",
    "def make_mse_cosine_loss(alpha):\n",
    "    def mse_cosine_loss(y_true, y_pred):\n",
    "        # y_pred = tf.clip_by_value(y_pred, clip_value_min=0, clip_value_max=127)\n",
    "        return alpha * (1 * cosine_similarity(y_true, y_pred)) + (1 - alpha) * mse(y_true, y_pred)\n",
    "    return mse_cosine_loss\n",
    "\n",
    "ALPHA = 0.15\n",
    "mse_cosine_loss = make_mse_cosine_loss(ALPHA)\n",
    "SAMPLE_LENGTH = 4\n",
    "# model = keras.models.load_model('saved_models/mvi-v2-2023-07-19_00-33_57-h4-e5-mse_cosine_loss-alpha0.15-LSTM-luong_attention-MAESTRO.h5', custom_objects={'mse_cosine_loss': mse_cosine_loss})\n",
    "model = keras.models.load_model('saved_models/mvi-v2-2023-07-20_13-00_56-h4-e5-mse_cosine_loss-alpha0.15-m0.60-LSTM-luong_attention-MAESTRO.h5', custom_objects={'mse_cosine_loss': mse_cosine_loss})\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing starts here\n",
    "\n",
    "from pathlib import Path\n",
    "from subprocess import Popen, PIPE\n",
    "\n",
    "DEMO_DATASET_PATH = '../demo_dataset'\n",
    "UTIL_PATH = '../util'\n",
    "\n",
    "# Generate csv from midi files\n",
    "org_filenames = glob.glob(f'{DEMO_DATASET_PATH}/*.midi')\n",
    "org_filenames.extend( glob.glob(f'{DEMO_DATASET_PATH}/*.mid'))\n",
    "for org_filename in org_filenames:\n",
    "    if org_filename.endswith('predicted.midi'):\n",
    "        print('Skipping predicted midi')\n",
    "        continue\n",
    "    if (CURRENT_OS == 'Windows'):\n",
    "        midi2csv_filename = f'{UTIL_PATH}/midi2csv.exe'\n",
    "    else:\n",
    "        midi2csv_filename = f'{UTIL_PATH}/midi2csv'\n",
    "    process = Popen([midi2csv_filename, org_filename], stdout=PIPE, universal_newlines=True)\n",
    "    (output, err) = process.communicate()\n",
    "    exit_code = process.wait()\n",
    "\n",
    "    print(output)\n",
    "    print(f'midi2csv exit code: {exit_code}')\n",
    "    if exit_code == 0:\n",
    "        a = 1\n",
    "    else:\n",
    "        print(f'midi2csv is not executed correctly (exit code: {exit_code}')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dataset_metadata_filepath = f'dataset32-MAESTRO-len{SAMPLE_LENGTH}.json'\n",
    "# model_dataset_metadata_filepath = f'dataset32-chien2021-len{SAMPLE_LENGTH}.json'\n",
    "import json\n",
    "\n",
    "# read dataset metadata from json file\n",
    "with open(model_dataset_metadata_filepath, 'r') as f:\n",
    "    dataset_metadata = json.load(f)\n",
    "    # time_min = dataset_metadata['time_min']\n",
    "    # time_max = dataset_metadata['time_max']\n",
    "    train_time_diff_min = dataset_metadata['train_time_diff_min']\n",
    "    train_time_diff_max = dataset_metadata['train_time_diff_max']\n",
    "    note_num_min = dataset_metadata['note_num_min']\n",
    "    note_num_max = dataset_metadata['note_num_max']\n",
    "    note_num_diff_min = dataset_metadata['note_num_diff_min']\n",
    "    note_num_diff_max = dataset_metadata['note_num_diff_max']\n",
    "    length_min = dataset_metadata['length_min']\n",
    "    length_max = dataset_metadata['length_max']\n",
    "    velocity_min = dataset_metadata['velocity_min']\n",
    "    velocity_max = dataset_metadata['velocity_max']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_train = ['time_diff', 'note_num', 'length', 'note_num_diff', 'low_octave'] # , 'time']\n",
    "columns_label = ['velocity']\n",
    "\n",
    "def divide_list(l, n, overlapping_window=0):\n",
    "    for i in range(0, len(l) - n + 1, n - overlapping_window):\n",
    "        yield l[i:i + n]\n",
    "    if len(l) % n != 0 and len(l) % n < n:\n",
    "        yield l[-(len(l) % n):]\n",
    "\n",
    "\n",
    "FEATURE_NUM = 5\n",
    "FLOAT_DTYPE = np.float32\n",
    "def pad_data(data, feature_num):\n",
    "    if (len(data[-1]) != SAMPLE_LENGTH):\n",
    "        # print(f'Length of last array: {len(data[-1])}')\n",
    "        last_array = data.pop()\n",
    "        # print(f'before padding: {last_array}')\n",
    "        zero_array = np.zeros((SAMPLE_LENGTH - len(last_array), feature_num), dtype=np.float32)\n",
    "        last_array = np.concatenate((last_array, zero_array))\n",
    "        # print(f'after padding: {last_array}')\n",
    "        data.append(last_array)\n",
    "        # print(f'Length of last array (after padding): {len(data[-1])}')\n",
    "    return data\n",
    "\n",
    "def make_dataset(csv_data, columns_train, columns_label):\n",
    "    dataset_entire_input = np.empty((0, SAMPLE_LENGTH, FEATURE_NUM), dtype=FLOAT_DTYPE)\n",
    "    dataset_entire_label = np.empty((0, SAMPLE_LENGTH, 1), dtype=FLOAT_DTYPE)\n",
    "\n",
    "    data_input_raw = np.array(csv_data[columns_train], dtype=FLOAT_DTYPE)\n",
    "    data_label_raw = np.array(csv_data[columns_label], dtype=FLOAT_DTYPE)\n",
    "\n",
    "    # normalize the time difference\n",
    "    data_input_raw[:, 0] = (data_input_raw[:, 0] - train_time_diff_min) / (train_time_diff_max - train_time_diff_min)\n",
    "    # normalize the note number\n",
    "    data_input_raw[:, 1] = (data_input_raw[:, 1] - note_num_min) / (note_num_max - note_num_min)\n",
    "    # normalize the length\n",
    "    data_input_raw[:, 2] = (data_input_raw[:, 2] - length_min) / (length_max - length_min)\n",
    "    data_input_raw[:, 3] = (data_input_raw[:, 3] - note_num_diff_min) / (note_num_diff_max - note_num_diff_min)\n",
    "    # normalize the time\n",
    "    # data_input_raw[:, 5] = (data_input_raw[:, 5] - time_min) / (time_max - time_min)\n",
    "    # normalize only the velocity\n",
    "    data_label_raw[:, 0] = (data_label_raw[:, 0] - velocity_min) / (velocity_max - velocity_min)\n",
    "\n",
    "    dataset_input = list(divide_list(data_input_raw, SAMPLE_LENGTH))\n",
    "    dataset_input = pad_data(dataset_input, FEATURE_NUM)\n",
    "    dataset_input = np.array(dataset_input, dtype=FLOAT_DTYPE)\n",
    "    dataset_entire_input = np.vstack((dataset_entire_input, dataset_input))\n",
    "\n",
    "    dataset_label = list(divide_list(data_label_raw, SAMPLE_LENGTH))\n",
    "    dataset_label = pad_data(dataset_label, 1)\n",
    "    dataset_label = np.array(dataset_label, dtype=FLOAT_DTYPE)\n",
    "    dataset_entire_label = np.vstack((dataset_entire_label, dataset_label))    \n",
    "    \n",
    "    return dataset_entire_input, dataset_entire_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing data for csv\n",
    "def generate_csv(csv_file, filename, columns_input, result):\n",
    "    data_demo_input = np.array(csv_file[columns_input], dtype=int)\n",
    "    # Get the length of data_demo_input and result_augmented\n",
    "    length_data_demo = len(data_demo_input[:, 0])\n",
    "    length_result_demo = len(result.reshape(-1))\n",
    "    # print(length_data_demo, length_result_demo)\n",
    "    if (length_data_demo < length_result_demo):\n",
    "        result = result[:length_data_demo]\n",
    "\n",
    "    data_demo_velocity = np.array(np.round(result), dtype=int)\n",
    "\n",
    "    dataframe = pd.DataFrame({'time': data_demo_input[:, 0], \n",
    "                            'time_diff': data_demo_input[:, 1], \n",
    "                            'note_num': data_demo_input[:, 2], \n",
    "                            'length': data_demo_input[:, 3], \n",
    "                            'note_num_diff': data_demo_input[:, 4],\n",
    "                            'low_octave': data_demo_input[:, 5],\n",
    "                            'velocity': data_demo_velocity})\n",
    "    dataframe.to_csv(filename, index=False)\n",
    "columns_full_input = ['time', 'time_diff', 'note_num', 'length', 'note_num_diff', 'low_octave']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## util/csv2midi ??_predicted.csv ??.midi\n",
    "def csv2midi(csv_filename, midi_original_filename):\n",
    "    if (CURRENT_OS == 'Windows'):\n",
    "        csv2midi_filename = f'{UTIL_PATH}/csv2midi.exe'\n",
    "    else:\n",
    "        csv2midi_filename = f'{UTIL_PATH}/csv2midi'\n",
    "\n",
    "    process = Popen([csv2midi_filename, csv_filename, midi_original_filename], stdout=PIPE, universal_newlines=True)\n",
    "    (output, err) = process.communicate()\n",
    "    exit_code = process.wait()\n",
    "\n",
    "    print(output)\n",
    "    print(f'csv2midi exit code: {exit_code}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "\n",
    "extension = 'csv'\n",
    "os.chdir(DEMO_DATASET_PATH)\n",
    "test_csv_filenames = glob.glob('*.{}'.format(extension))\n",
    "csv_files_demo = []\n",
    "for filename in test_csv_filenames:\n",
    "    df = pd.read_csv(filename, index_col=None, header=0)\n",
    "    df.attrs['filename'] = filename\n",
    "    csv_files_demo.append(df)\n",
    "\n",
    "os.chdir(current_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = 0.1391 # mae_predict\n",
    "# b = 0.1446 # sd_predict\n",
    "# c = 0.6993 # f1\n",
    "# x = (b * (2 - 6 * a - c)) / (c * (1 - 3 * a))\n",
    "# print(f'x: {x}') # 0.16540127369027316\n",
    "\n",
    "# rv_sd = 0.1446\n",
    "# true_sd = 0.1654\n",
    "# mae_norm = 0.1391\n",
    "\n",
    "# f1 = 2 * (((rv_sd / true_sd) * (1 - (3 * mae_norm))) / ((rv_sd / true_sd) + (1 - (3 * mae_norm))))\n",
    "# print(f'f1: {f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_list = []\n",
    "mae_list = []\n",
    "mse_list = []\n",
    "sd_list = []\n",
    "\n",
    "for demo_input_csv in csv_files_demo:\n",
    "    demo_dataset_input, demo_dataset_label = make_dataset(demo_input_csv, columns_train, columns_label)\n",
    "    demo_dataset_result = model.predict(demo_dataset_input)\n",
    "    \n",
    "    # print(demo_dataset_result.shape, demo_dataset_label.shape)\n",
    "    result_velocity = demo_dataset_result.reshape(-1) * velocity_max\n",
    "    true = demo_dataset_label.reshape(-1) * velocity_max\n",
    "    result_velocity = result_velocity.round()\n",
    "    result_velocity = result_velocity.clip(0, velocity_max)\n",
    "    \n",
    "    # mae = np.mean(np.abs(result_velocity - true))\n",
    "    # print(f'MAE: {mae}, MAE(normalized): {mae / 127}')\n",
    "    rv_norm = result_velocity / 127\n",
    "    true_norm = true / 127\n",
    "    mae_norm = np.mean(np.abs(rv_norm - true_norm))\n",
    "    mse_norm = np.mean((rv_norm - true_norm) ** 2)\n",
    "    mae_list.append(mae_norm)\n",
    "    mse_list.append(mse_norm)\n",
    "    \n",
    "    rv_sd = np.std(rv_norm)\n",
    "    true_sd = np.std(true_norm)\n",
    "    f1 = 2 * (((rv_sd / true_sd) * (1 - (3 * mae_norm))) / ((rv_sd / true_sd) + (1 - (3 * mae_norm))))\n",
    "    f1_list.append(f1)\n",
    "    sd_list.append(rv_sd)\n",
    "        \n",
    "    original_filename = demo_input_csv.attrs['filename']\n",
    "    original_shortname = f'{Path(original_filename).stem}'\n",
    "    original_extension = Path(original_filename).suffix\n",
    "    \n",
    "    print(f'original_filename: {original_shortname}')\n",
    "    print(f'MAE(normalized): {mae_norm}, SD(predict): {rv_sd}, SD(true): {true_sd}, f1: {f1}')\n",
    "\n",
    "    # break\n",
    "    # continue\n",
    "\n",
    "    demo_shortname = f'{original_shortname}_predicted'\n",
    "    demo_output_csv_filename = f'{DEMO_DATASET_PATH}/{demo_shortname}.csv'\n",
    "    generate_csv(demo_input_csv, demo_output_csv_filename, columns_full_input, result_velocity)\n",
    "\n",
    "    # print(f'original_filename: {original_shortname}')\n",
    "    \n",
    "    # util/csv2midi ??_predicted.csv ??.midi\n",
    "    midi_original_filename = f'{DEMO_DATASET_PATH}/{original_shortname}{original_extension}'\n",
    "    csv2midi(demo_output_csv_filename, midi_original_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print mean f1, mae, mse score\n",
    "print(f'f1: {np.mean(f1_list)}, mae: {np.mean(mae_list)}, mse: {np.mean(mse_list)}, sd: {np.mean(sd_list)}')\n",
    "print(f'f1: {np.max(f1_list)}, mae: {np.max(mae_list)}, mse: {np.max(mse_list)}, sd: {np.max(sd_list)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
